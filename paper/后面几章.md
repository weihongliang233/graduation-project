理论介绍与实验结果

CNN

卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元,对于大型图像处理有出色表现。

在 CNN 出现之前，图像对于人工智能来说是一个难题，有2个原因：

图像需要处理的数据量太大，导致成本很高，效率很低
图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高

而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。
人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，然后进一步抽象（大脑进一步判定该物体的种类）。我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。

典型的 CNN 由3个部分构成：

卷积层
池化层
全连接层

卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。

![](E:\GitHubLocalRepository\graduation-project\paper\2022-03-19-22-48-04.png)



Noise2Noise

无监督方法不需要高质量、低噪声的数据（真实数据）。这种方法试图利用所呈现数据的内部统计数据来执行图像恢复。下面我们将介绍几种不需要真实数据的（半）监督和无监督网络训练方法。

```
如前所述，传统的 CARE 网络需要在成对的图像 (x, y) 上进行训练，其中 x 是有噪声的图像（由真实信号 s 和叠加噪声 n 组成），并且很好地逼近了真值信号 y。如上所述，训练 CARE 网络 E 使得损失函数 L(E(x), y) 被最小化。相比之下，Noise2Noise (Lehtinen et al., 2018) 是一种不需要干净的地面实况的神经网络训练方案。相反，知道两个噪声图像 x1¼s+n1 和 x2¼s+n2 就足够了，这样 n1 和 n2 是独立的并且具有零均值。高斯噪声和泊松噪声都满足了这些要求，已知它们是光显微镜和电子显微镜中的两个主要噪声源
```

Noise2Noise 训练很容易实现。不是最小化对可用真实值 y 的损失，而是简单地最小化 L(E(x1), x2)。在训练过程中，由于 n1 和 n2 的独立性，要求网络实现不可能的任务，即将 x1 的噪声像素值映射到 x2 的噪声像素值。根据噪声的种类，必须使用合适的损失函数，并且手头有足够的数据，网络将学会预测高质量的图像恢复（Lehtinen 等人，2018 年）。因此，只要可以获得具有独立噪声的图像对，即使在没有真实情况的情况下，Noise2-Noise 也可以进行 CARE 网络训练

Noise2Void

虽然 Noise2Noise 可以在没有真实情况的情况下训练 CARE 网络，但（Krull 等人，2018 年）最近的工作展示了如何启用对单个图像采集的 CARE 训练。这种训练机制需要从单个噪声图像中获取输入数据和目标数据。受不存在的目标图像的启发，这种训练方法称为 Noise2Void。卷积神经网络 (CNN) 中的所有节点都对输入像素的子集进行操作，统称为它们的感受野。因此，CARE 网络最后一层的输出节点不仅从单个输入像素接收信息，而且从围绕像素的整个图像块接收信息后，预测目标像素值。 Noise2Void 的基本思想是取出感受野中心的单个像素，从而创建一个“盲点”。然后，我们可以使用这个移除的像素值作为学习网络的目标，该网络将预测隐藏在盲点中的值。请注意，此目标不是真实像素值，因为它本身仅取自嘈杂的输入图像。因此，这里也需要证明成功的 Noise2Noise 训练的相同论点。要使 Noise2Void 训练成功，必须满足两个条件。数据必须在统计上相互依赖，这意味着知道一个像素的周围环境可以让观察者预测该像素的值。此外，图像中的噪声需要与像素无关（给定信号）。一种类似的方法，称为 Noise2Self，由 (Batson & Royer, 2019) 独立开发。他们的工作提供了自监督去噪的广义理论背景，可用于校准任何参数化去噪算法——从中值滤波器的单个超参数到深度神经网络的数百万权重。最近，（Laine 等人，2019 年）展示了如何在不遗漏盲点的情况下构建神经网络。此外，他们展示了如何在仅存在高斯噪声的情况下进一步改善降噪



去噪结果的评价指标：

对于图片降噪的效果，人们一般从客观评价和主观评价两方面进行评价。

客观评价：

在主观评价中,往往会根据不同的观察者而产生不同的评测结果,这样观察结果极度依赖,于观察者,所以使用主观评价对图像进行评测并不严谨。所以本文需要 
更加注重于客观的评价指标,通过对图像的定性分析来评测图像质量。使用客观评价对图像进行评测的方法主要为全参考评价和部分参考评价以及无参考评价。

全参考评价是对目标的像素进行逐一对照的方式对图像质量进行评价。通过计算待评价图,像和参考图像之间的差距,参考图像越相似,图像质量越好,这个差
异会越小,在论文中常用,的是峰值信噪比[451、均方根误差等方法,其中峰值信噪比是最常用的图像客观评价规则。

部分参考评价是通过两张图像的部分信息 进行评测,从而得出两张图像的差别大小,论文,常见的方法有结构相似性[461、基于像素统计的评价方法等。

无参考评价是不需要参考图像的,它通过大量的样本学习样本的数据分布,通过提取相应,的特征给出评价数值,从而完成评价,在传统生成对抗网络中判别模型就可以看作是这种指标,来对图像进行判定。

本文 实验的目标样本是冷冻电镜图像,在对冷冻电镜图像进行去噪的结果,本文希望在有,效去噪的同时,保持原始信号的完整性。接下来将介绍本文使用的几个客观 
评测指标。

均方误差,它在参数估计中通常被用来计算参数估计值和参数真实值之间的差平方的期望,值,在图像上则是对两幅图像在对应的像素位置的像素值进行运算,同
时求出整幅图中的差平,方的期望值。两幅图像越是接近则它们两幅图像的均方误差就越接近于0,均方误差的计算方,法如式(4-2)所示:

![image-20220329151254521](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151254521.png)

4.峰值信噪比(Peak Signal to Noise Ratio, PSNR),峰值信噪比,是最普遍,最广泛使用的评价图像质量的客观量测方法,通常为了衡量经过,处理的图像的质量
,本文会参考PSNR质量均衡处理器是否优秀,对于单通道图像的PSNR计算,方法如式(4-5):

![image-20220329151621524](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151621524.png)

5.结构相似性(Structural Similarity Index, SSIM),结构相似性用于衡量两幅图像相似度。这个指标首先由德州大学提出,结构相似性的范围,为0到。当两张
图像相同时, SSIM的结果会等于1。所以两张图像越像SSIM值就越接近于1,,两张图像越不接近时其SSIM值就越接近于0.SSIM的计算方法如式(4-6)所示:       

![image-20220329151733217](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151733217.png)

实验

实验平台：兰州大学高性能计算中心

硬件条件：

软件条件：

实验步骤：

用不同参数添加噪声，产生数据集

跑模型

算参数

实验结果：

原图：

![image-20220329155957998](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329155957998.png)

加噪声：

![image-20220329170146389](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329170146389.png)

![image-20220329171305636](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329171305636.png)



网络结构层次

层和块

之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。 在这里，整个模型只有一个输出。 注意，单个神经网络 （1）接受一些输入； （2）生成相应的标量输出； （3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。

然后，当考虑具有多个输出的网络时， 我们利用矢量化算法来描述整层神经元。 像单个神经元一样，层（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。 当我们使用softmax回归时，一个单层本身就是模型。 然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。

对于多层感知机而言，整个模型及其组成层都是这种架构。 整个模型接受原始输入（特征），生成输出（预测）， 并包含一些参数（所有组成层的参数集合）。 同样，每个单独的层接收输入（由前一层提供）， 生成输出（到下一层的输入），并且具有一组可调参数， 这些参数根据从下一层反向传播的信号进行更新。

事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由*层组*（groups of layers）的重复模式组成。 这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛 的识别和检测任务 [[He et al., 2016a\]](https://zh.d2l.ai/chapter_references/zreferences.html#he-zhang-ren-ea-2016)。 目前ResNet架构仍然是许多视觉任务的首选架构。 在其他的领域，如自然语言处理和语音， 层组以各种重复模式排列的类似架构现在也是普遍存在。

为了实现这些复杂的网络，我们引入了神经网络*块*的概念。 *块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。 使用块进行抽象的一个好处是可以将一些块组合成更大的组件， 这一过程通常是递归的，如 [图5.1.1](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html#fig-blocks)所示。 通过定义代码来按需生成任意复杂度的块， 我们可以通过简洁的代码实现复杂的神经网络。

从编程的角度来看，块由*类*（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。 注意，有些块不需要任何参数。 最后，为了计算梯度，块必须具有反向传播函数。 在定义我们自己的块时，由于自动微分（在 [2.5节](https://zh.d2l.ai/chapter_preliminaries/autograd.html#sec-autograd) 中引入） 提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。

Conv

MaxPool

激活函数

通过矩阵X∈Rn×d 来表示n个样本的小批量， 其中每个样本具有d个输入特征。 对于具有h个隐藏单元的单隐藏层多层感知机， 用H∈Rn×h表示隐藏层的输出， 称为*隐藏表示*（hidden representations）。 在数学或代码中，H也被称为*隐藏层变量*（hidden-layer variable） 或*隐藏变量*（hidden variable）。 因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重W(1)∈Rd×h 和隐藏层偏置b(1)∈R1×h 以及输出层权重W(2)∈Rh×q 和输出层偏置b(2)∈R1×q。 形式上，我们按如下方式计算单隐藏层多层感知机的输出 O∈Rn×q：

(4.1.1)H=XW(1)+b(1),O=HW(2)+b(2).

注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？ 你可能会惊讶地发现：在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。

我们可以证明这一等价性，即对于任意权重值， 我们只需合并隐藏层，便可产生具有参数 W=W(1)W(2) 和b=b(1)W(2)+b(2) 的等价单层模型：

(4.1.2)O=(XW(1)+b(1))W(2)+b(2)=XW(1)W(2)+b(1)W(2)+b(2)=XW+b.

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后对每个隐藏单元应用非线性的*激活函数*（activation function）σ。 激活函数的输出（例如，σ(⋅)）被称为*活性值*（activations）。 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：

(4.1.3)H=σ(XW(1)+b(1)),O=HW(2)+b(2).

由于X中的每一行对应于小批量中的一个样本， 出于记号习惯的考量， 我们定义非线性函数σ也以按行的方式作用于其输入， 即一次计算一个样本。 我们在 [3.4.5节](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#subsec-softmax-vectorization)中 以相同的方式使用了softmax符号来表示按行操作。 但是在本节中，我们应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。 这意味着在计算每一层的线性部分之后，我们可以计算每个活性值， 而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。

为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如H(1)=σ1(XW(1)+b(1))和H(2)=σ2(H(1)W(2)+b(2))， 一层叠一层，从而产生更有表达能力的模型。

下采样   插值   上采样

在图像处理中，我们有时需要将图像放大，即*上采样*（upsampling）。 *双线性插值*（bilinear interpolation） 是常用的上采样方法之一，它也经常用于初始化转置卷积层。

为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。 首先，将输出图像的坐标(x,y)映射到输入图像的坐标(x′,y′)上。 例如，根据输入与输出的尺寸之比来映射。 请注意，映射后的x′和y′是实数。 然后，在输入图像上找到离坐标(x′,y′)最近的4个像素。 最后，输出图像在坐标(x,y)上的像素依据输入图像上这4个像素及其与(x′,y′)的相对距离来计算。



训练过程：

训练器

在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定n个样本的训练数据集，我们假设fi(x)是关于索引i的训练样本的损失函数，其中x是参数向量。然后我们得到目标函数

(11.4.1)f(x)=1n∑i=1nfi(x).

x的目标函数的梯度计算为

(11.4.2)∇f(x)=1n∑i=1n∇fi(x).

如果使用梯度下降法，则每个自变量迭代的计算代价为O(n)，它随n线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。

随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引i，其中i∈{1,…,n}，并计算梯度∇fi(x)以更新x：

(11.4.3)x←x−η∇fi(x),

其中η是学习率。我们可以看到，每次迭代的计算代价从梯度下降的O(n)降至常数O(1)。此外，我们要强调，随机梯度∇fi(x)是对完整梯度∇f(x)的无偏估计，因为

(11.4.4)Ei∇fi(x)=1n∑i=1n∇fi(x)=∇f(x).

这意味着，平均而言，随机梯度是对梯度的良好估计。



loss函数



样本

batch_size

epoch



结果分析



