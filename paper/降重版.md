[TOC]



## 绪论 第一章

### 意义

生物大分子三维结构与功能之间的关系是现代生命科学、材料学和应用物理的一个重要研究领域。 

在病毒研究领域，了解病毒的分子结构有助于从分子层面理解其组装和行使功能的机制；在医学和药物研发领域，对生物分子结构的研究已广泛应用于基于结构的新药开发；在人体内代谢的研究中，其有助于理解人体蛋白质结构与其相互作用机制。

冷冻电镜技术结合了冷冻电子显微技术三维重构技术，能将含水生物大分子迅速冷冻至液氮或液氦温度，避免生物大分子结构在产生晶态冰的过程中被破坏；非晶冰还使得保证冷冻电镜环境下生物分子中的水得以保存，使其接近于生理活性状态。以上特点使得冷冻电镜可以在原子分辨率水准下研究生物大分子的结构。为了防止样品在观察过程中受到高能电子束的辐射损伤，一般只能采用低剂量的电子照射，这使得在冷冻电镜图像的信噪比很低，因此，图像降噪是冷冻电镜解析生物大分子的重要环节。

传统的图像去噪方法一般采用均值滤波、高斯滤波、维纳滤波等滤波技术。这些方法在冷冻电镜图片降噪中被广泛使用；高斯滤波器被用于在单颗粒分析过程中对颗粒进行识别；维纳滤波器被用于生物大分子结构的三维重建；迭代中值滤波器被用于对冷冻电子断层结构重建图进行特征提取、模式识别。虽然滤波器降噪的应用广泛，但是每种滤波器都存在其局限性：高斯滤波后的图像会变得更加模糊；中值滤波器受到滑动模板尺寸的限制：模板过大会破坏图像细节；模板过小则不能有效去除噪声。总体而言，现有的滤波器种类繁多，但是每种滤波器都有一定的缺点和局限。

近年来，除开传统的滤波器去噪，深度学习技术也逐渐被应用于图片降噪处理。深度学习是机器学习的分支，是一种以人工神经网络为架构，对大量数据进行表征学习的算法。通过设计网络结构，使用大量数据进行参数训练，深度神经网络模型能够以较高的准确率完成图像增强，图像降噪，特征提取等图像处理工作。相比传统图片降噪技术，深度学习更加适合冷冻电镜这类大规模数据集，其神经网络参数可变，随着训练数据量增大而自动优化，减少了需要人力操作的步骤；同时，深度学习还可引入自动图像分割等功能，更好地完成颗粒挑选等工作；根据冷冻电镜图像的特征设计神经网络层次结构，可以更加充分地利用图像信息，保留图片细节。目前，深度学习降噪更多地被用于通用图片处理，针对冷冻电镜图像设计的深度神经网络主要还停留于基础的卷积神经网络（CNN）。通过结合冷冻电镜的图像特征，借鉴现有的神经网络结构，参考传统滤波降噪方法，我们可以设计出更加适合冷冻电镜图像降噪处理的深度学习模型。

本课题尝试基于深度学习方法，对比现有图片降噪技术，提出针对冷冻电镜图片降噪新思路，助力生物大分子高分辨三维结构的解析，从而为揭示生物大分子三维结构与功能的关系奠定基础。

## 第2章 原理

### 噪声和去噪的原理

由于环境、传输通道等因素的影响，图像在采集、压缩和传输过程中不可避免地受到噪声的污染，导致图像信息失真和信息丢失。

图像去噪是从有噪声的图像中去除噪声，从而恢复真实的图像。然而，由于噪声、边缘和纹理是高频分量，在去噪过程中很难区分它们，去噪后的图像不可避免地会丢失一些细节。总体而言，在去噪过程中从噪声图像中恢复有意义的信息以获得高质量的图像是当今的一个重要问题。

事实上，图像去噪是一个经典的问题，尽管针对图像去噪已经有相当多的研究工作，它仍然是一项具有挑战性和开放性的任务。从数学的角度来看，主要原因是图像去噪是一个逆问题，其解并不唯一。

在数学上，图像去噪问题可以建模如下：
y = x + n
其中y是观察到的噪声图像，x是未知的干净图像，n表示噪声分量，降噪的目的是降低自然图像中的噪声，同时尽量减少原始特征的损失，提高信噪比（SNR）。由于从等式求解干净的图像 x是一个不适定问题，我们无法从有噪声的图像模型中得到唯一解。图像去噪的主要挑战如下：

1.物体平坦的区域的复原效果应该是光滑的
2.边缘应受到保护而不会模糊
3.在保留图像原本纹理的同时不会生成新的纹理

一般来说，图像去噪方法可以大致分为空间域方法和变换域方法，在接下来的章节中将更详细地介绍。

### 机器学习和深度学习的原理

机器学习是人工智能的一个分支，通过对大量数据进行学习，从而模仿人类完成分类、预测等任务。

所有机器学习的任务都需要如下要素：
1.可以供我们进行训练的数据
2.对数据进行运算的模型
3.一个目标函数，用于对模型的真实性和适用性进行量化检验
4.通过调整模型参数而实现模型优化的算法

在机器学习中，训练过程包括如下步骤：

1.从随机初始化参数的模型开始，该模型基本上不具备智能

2.获取数据样本（图像、标签等）

3.调整参数，使得对于训练集，模型能给出的预测效果可以与样本相符

4.重复第二步和第三步，知道模型能以较好的效果完成指定任务



传统的机器学习算法一般可根据是否需要数据标签而分为有监督学习和无监督学习。一个典型的有监督式学习任务在观察完一些事先标记过的训练示例（输入和预期输出）后，训练出来的模型可以对新的输入进行预测。而无监督学习算法不需要给定事先标记过的训练样本，可以自动对输入的数据进行分类或预测。

深度学习是机器学习的分支，通过构建深层次的人工神经网络，能更好的提取出数据中的特征。深度学习应用于图像处理时，输入数据一般是一个多维数组，而深度学习技术的主要任务就是从中抽取出一系列特征，例如图像实体的边界、特定形状的区域等。比起比起传统的机器学习，深度学习算法更多地倾向于无监督或半监督式的特征学习，能利用高效的特征提取算法来代替手工获取图像特征，从而更好地从大规模未标记的数据中学习其特征。

目前未知学界已经提出多种深度学习框架，如深度神经网络，卷积神经网络和循环神经网络等。这些网络模型已被大量地运用到图像处理、计算机视觉等领域并取得巨大成功。

## 第3章 前人理论的阐述和实验复现

### CNN

卷积神经网络主要由卷积层、池化层、全连接层三种网络层组成。

卷积层的思想来源于对人和动物视觉系统的研究。动物视觉系统在处理图像时，是分层抽象的，其最先关注的是颜色和亮度，然后是直线、边缘、点等局部的细节特征，再之后是纹理、集合形状等更加复杂的结构和信息，而后形成对物体的整体感知。

卷积层模仿的正式这个特征抽取的过程：在输入图像上滑动不同的卷积核并执行一定的运算，从而产生特征图(feature map)。在滑动过程中，卷积核与图像之间会执行卷积运算，将当前感受域中的元素乘以对应的卷积核系数，最终求和，从而将感受域内的信息投影到特征图上的一个元素。卷积核的尺寸比输入图像小得多，或重叠或平行地作用于输入图像上。进行一轮卷积核滑动下来可以得到一张特征图，将此过程不断重复即可得到多张平行地特征图，每一张都从上一张得到更抽象的信息，从而提取物体从微观到宏观的特征。

池化是卷积神经网络中另一个重要网络层。池化层的主要作用是不断缩小数据的空间大小，从而使得参数数量和计算量都大大下降，并且在一定程度上可以控制过拟合的现象。以常用的最大池化为例，它的主要操作是将输入的图像分割为若干大小的区域，每个子区域输出其最大值。和卷积层类似，步幅也是池化层的重要参数。当池化步幅为2时，池化窗口的形状为2*2，从窗口中的4个数中取出最大值，能够减少75%的数据量，减轻运算负担。这种机制能够有效工作的主要原因是，在一副图像中，特征区域往往只集中于其中的一小块；数值较大的像素点对图像的影响远超过数值较小的像素点。同时，池化层在某种程度上为图像处理提供了另一种形式的平移不变形：卷积核是一种特征发现器，可以用它较容易的发现图像的边缘细节。大但是卷积层发现的特征通常过于精确，通过池化操作我们可以降低其对边缘的敏感性，从而改善过拟合的现象。

全连接层负责将卷积层和池化层的处理映射到最终的输出上。全连接层的神经元与其前一层的所有神经元都有连接，用于把之前提取的特征综合起来输出为一个值。

### Noise2Noise

以CNN为代表的去噪算法可以归结为如下问题的解决：

![image-20220406175935532](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220406175935532.png)

上面式子中，f是网络模型对应的函数，x是输入的带噪声图像；y是理想的无噪图像，L是衡量降噪结果与真实图像之间差异的损失函数。上式的数学意义是，对于所有的图像对(x,y)，通过调整网络参数theta，使得Loss函数的均值最小。

在以CNN为核心的去噪算法中，人们一般需要知道完全洁净无噪声的原始数据和加了噪声的加噪数据；两者之差即为需要学习的噪声特征。这样的过程可以被归为有监督学习；然而在处理冷冻电镜图像时，人们往往难以获得完全无噪声的原始数据，由此需要发展出完全利用噪声数据来进行学习的无监督网络训练方法。

Noise2Noise正是一种无需洁净数据集的网络训练方案。其主要思想接近于通过多次求平均值来减小测量误差的处理方法。Noise2Noise方法的数学表述如下：

![image-20220406195249614](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220406195249614.png)

xi和yi代表对同一个实体的两次摄像，每次摄像都存在噪音，但是对于x和y来说其噪音是独立同分布的。当我们对多个实体进行摄像，即可得到多个(x,y)对，这些图像对构成了优化网络参数所需的训练集。如果Loss函数选取合适，则经过多轮训练后得出的网络模型f满足如下特性：若我们构造出另一组图片集z，使得z的噪声均值跟y的噪声均值相等，那么用(x,y)点对训练出来的网络模型将完全等价于用(x,z)点对的网络模型；换言之，如果我们使用带有噪声但是噪声均值为0的数据集进行训练，得出来的模型跟噪声-洁净集的训练结果完全等价。

### Noise2Void

Noise2Void方法是对Noise2Noise方法的进一步拓展。Noise2Noise摆脱了对于理想数据集的依赖，Noise2Void则摆脱了对数据对的依赖，仅凭一张图片来训练网络实现去噪。

在一般的CNN训练过程中，网络输出的每一个像素都对应一个感受野，也就是感受野范围内所有像素都对输出值有影响。而在Noise2Noise的训练中采用的是盲点网络：这种网络会忽略感受野的中心值，其中心就是一个盲点。由此，网络的输出值受到其周围所有像素的影响（除了自身位置的输入）。

该网络的基本假设是：同一张图片的不同像素上的噪声是不相关的。在训练过程中，盲点网络将试图用一个像素周围的其它像素来复原该像素，但是由于像素间的噪声互不相关，最终被复原出来的只有真实图像信息。而采用盲点网络而非全CNN网络，是因为如果将中心位置的像素也加入到训练中，会使得网络学习到恒等映射，将输入图片原样返回，从而达不到降噪的目的。

与Noise2Noise网络相比，其需要的数据更少，但是由于它使用的是与Noise2Noise类似的机制，所以其训练效果并不会比Noise2Noise网络更佳，反而会因为在训练中没有使用所有可用的图像信息，其准确率也会有所下降。

### 去噪效果的评价指标
图片降噪的效果有主观评价和客观评价两种评价方法。其中主观评价是指人工根据主观感受来对降噪效果进行评定，不同观察者往往得出不同的评价结论，使用主观评价对图像降噪效果进行评测并不严谨，本文将更加注重客观评价指标。
客观评价指标又分为：全参考评价、部分参考评价、无参考评价三类。

全参考评价需要完整的参考图片，通过对参考图片和降噪图片的每一个像素进行对比，计算其差异值。差异值越大，说明降噪效果越差；反之降噪效果则越好。图像去噪领域的论文中常用峰值信噪比来进行评价，这也是目前最广泛被使用的评价指标。

部分参考指标是指获取两幅图像的部分信息来进行对比，常用的方法是结构相似性比较。

无参考评价不需要参考图像。通过学习大量数据样本，提取相应特征得出评价值；生成对抗网路中的判别模型就是此类指标。

本文将使用三种数值评价指标：均方误差、峰值信噪比、结构相似性。

#### 均方误差
均方误差的计算公式如下：

![image-20220329151254521](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151254521.png)

其中x和y分别是去噪图像和参考图像，i代表图像像素点，最终结果是图像在对应位置的像素值差平方的期望。

#### 峰值信噪比
峰值信噪比是应用最广泛的客观评价指标，其值跟均方误差有关，具体计算公式如下：


![image-20220329151621524](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151621524.png)

#### 结构相似性
结构相似性的基本概念是：图像实体是高度结构化的，其相邻的像素之间具有很强的相关性。这种相关性反映了图像实体的结构信息。其数学定义是
![image-20220329151733217](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220329151733217.png)

scikit-image是一款被广泛使用的图像处理Python库，它包含了以上三种评价指标的代码实现。本文采用其中的xxx函数来进行计算。

### 实验环境

| 硬件    |      |
| ------- | ---- |
| CPU     |      |
| 内存    |      |
| GPU环境 |      |

| 软件         |      |
| ------------ | ---- |
| 操作系统     |      |
| 深度学习框架 |      |
| 其余python包 |      |

### 实验数据来源

本文的实验数据来源分为两部分：由GroEL蛋白理论分子结构生成的理论结构图像和冷冻电镜实验中拍摄的实验图。

#### 理论图像

尽管在真实的冷冻电镜实验条件下很难得到完全洁净的图像，但是根据已有的生物大分子结构理论，我们可以构造出洁净的理论图。本文中我们选取了GroEL蛋白为对象，生成其理论图像，对理论图像添加不同的噪声可以生成不同的人工噪声图。由此我们可以将理论图作为对照，对比人工加噪和去噪后的图像信息，从而评判降噪算法的效果。

#### 实验图像

本文采用的实验拍摄的冷冻电镜图像来源于公开的冷冻电镜图像数据集EMPIAR。

### 三种方法的对比

我们将对洁净图片和加噪后的图片进行分析，之后将加噪后的图片用三种方法去噪，分析其特点，并且通过前面阐述的几种客观指标来对降噪效果进行衡量对比。

| 指标/降噪方法 | Noised | 高斯滤波器 | N2N    | N2V    |
| ------------- | ------ | ---------- | ------ | ------ |
| MSE           | 0.990  | 0.033      | 0.015  | 0.010  |
| PSNR          | 0.039  | 14.807     | 17.982 | 19.958 |
| SSIM          | 0.001  | 0.0249     | 0.5543 | 0.073  |

原始图像：

![image-20220409204915014](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220409204915014.png)

Noised

![image-20220410163828247](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220410163828247.png)

滤波器去噪

![image-20220410163816322](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220410163816322.png)

![image-20220410163857406](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220410163857406.png)

N2V

![image-20220410163933396](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220410163933396.png)

图像对比：

经过滤波降噪后，图片整体上变得更加模糊。在实体区域，一个窗口中大多像素都是信号区；而在背景区域，一个窗口中除了部分噪声外大部分区域都是0值，因此降噪后实体区域更加明显；但是由于高斯滤波器采取的是加权均值，因此图像整体更加模糊。从三个评价指标看，高斯滤波器降噪后的图像相比带噪声图像，MSE更低，PSNR更高，说明它起到一定的降噪效果；SSIM值有所提高，说明降噪后图片与原图结构更加相似。从三项指标来看，降噪后图片确实比噪声图有所改善，但是观察图片我们发现，滤波器降噪丢失了原图的结构细节，整体更加模糊，同时背景噪声只是被平均化而非完全消除。

经过N2N方法降噪后，图片的三项指标比滤波降噪有很大提升。背景噪声几乎被完全消除，使得图中实体部分更加突出，同时实体中结构细节被保留，轮廓更加清晰，避免了滤波器降噪的模糊问题。



## 第四章 我设计的网络
一个深度学习任务由以下因素决定：网络模型的结构、训练网络模型所用的数据、训练时采用的损失函数、训练中使用的参数优化策略
### 网络结构层次

本文设计的网络架构主要包括四种类型的网络层：卷积层，池化层，非线性激活层，全连接层。网络整体由多个块组成，每个块包含以上四种类型的网络层的一层或多层。由于在卷积过程中，图片尺寸会缩小，因此图片在网络中会经过编码和解码过程。

编码器由4层组成，分别为enc1, enc2, enc3, enc4。
每一层enc可表述为：

enc = Sequential(Conv2d, Relu, MaxPool2d)
其中Conv2d表示卷积层，Relu表示Relu激活函数层，MaxPool2d表示最大池化层。三层呈序列排布。

每个enc的Relu层和MaxPool2d层都相同，不同的是每个enc的卷积层采取了不同的窗口大小。由于最佳窗口大小取决于图像细节的尺寸，无法在一开始便确定下来。所以窗口大小是多次尝试后，根据降噪效果和收敛速率等指标选取的。

解码器负责将图片像素增加。一张图片经过多个卷积层以后，像素数量会大大减小，每个像素承载的信息量也大大增加，保留了图像的主要特征。为了从主要特征中恢复出跟原图等量的像素，人们一般采用插值的方法。因此每个解码器都等价于一个插值函数。

带有噪声的图像经过编码器和解码器运算后，就得到相同尺寸的降噪图像。

以上是网络层次的具体构成，接下来阐述构造这个网络的思路，也就是为什么要设计这样的网络结构。


复杂的网络结构是由简单的“块”组成的。在神经网络模型中，进行一次向量化运算并得到对应输出的结构叫做层，多个层叠加起来组成块，块的重复叠合则组成了整个模型结构。在计算机视觉领域广泛使用的ResNet结构就有数百个层次，但是层次的种类并不多，这些曾是由相同的层组块重复叠加而成。ResNet架构在多种计算机视觉任务中都是首选架构，在其他的领域，如自然语言处理和语音处理，层组块重复排列的模式也普遍存在。事实证明，讨论比单个层大但比整个模型小的组件更有价值。将模型设计为多个相似的enc和dec采用的就是这种思想。

从编程的角度看，每个块的程序实现都是一个类(class)，每个表示神经网络层的类都必须定义一个将其输入转化为输出的前向传播函数，并且存储所需的参数。而为了计算梯度，每个块也必须定义反向传播函数。在编程实现中，深度学习的框架都为我们提供了自动微分的默认实现，因此在编写程序时我们只定义了前向传播函数和参数。

卷积层和池化层的作用之前已经提到，事实上，几乎每个图像处理的深度学习方法都使用了这两个层次来进行特征提取。而引入Relu函数层的主要目的是提高模型的非线性特性。以下将具体解释这个网络层的作用。
当我们构建深度网络模型是，我们都希望该模型具有足够的表达力，能够真实地反映输入和出之间的关系。神经网络模满足如下表达式：
f(x)=y
f是神经网络对应的函数，x和y对应输入和输出。一般来说，真实世界的f总是对应非线性的函数，这要求我们的网络模型也具有一定的非线性。最早期的网络模型是多层感知机模型，该模型的包含输入层、输出层和隐藏层。其中的每一层都对应线性函数；增加隐藏层的数目会增加参数量和计算量，然而事实证明，这并不会给模型带来改善：多层线性网络层可以被完全规约为具有更多参数的单层线性层，通过多个线性层叠加出来的模型仍然只具有线性表达力。
为了是模型具有线性特性，我们必须改造其中的层次，将隐藏层中的一层或多层替换为非线性函数。这种函数被称为激活函数。
非线性函数多种多样，目前主流深度学习使用的激活函数是Sigmoid函数、Tanh函数、Relu函数。
、
1.sigmoid函数
sigmoid函数又名logitstic函数，它的取值范围为(0,1)，数学表达式：

图像是：

其优点是：其输出范围有限，可以作为输出层使用。
其主要的缺点是饱和问题：当输入值的绝对值非常大时，函数会趋向于常数，形状变得非常平缓，因此对于输入的变化变得不敏感。

2. Tanh函数
数学表达式：
图像：
3. Relu函数
Relu函数整流线性函数，它是现代神经网络中应用最广泛的激活函数。
数学表达式：
图像：
其优点是使用Relu函数的模型，训练收敛速率更快。
在x>0的区域上不会出现前两者的梯度饱和问题。

鉴于Relu函数是深度学习最常用的激活函数，且具有以上诸多优点，因此我们的网络模型也采用它作为激活函数。

以上介绍了我们网络模型中出现的神经层种类。接下来解释解码器的构成。
图像经过卷积层的处理可以看做是经历了一次下采样，也即从大量数据中采样出少部分数据；而解码器则是一个上采样的过程。双线性插值是一种常用的上采样方法。
双线性插值的含义如下：
双线性插值直接找到原图像的对应点，将数值赋予新矩阵对应点；新矩阵的点比原矩阵更密集，当遇到没有没有原矩阵对应的点时，采取其最邻近的点作线性插值，算出新值赋予新矩阵。


### 训练数据
CNN模型需要干净数据集和噪声数据集对比，Noise2Noise方法需要同一实体的多张带噪声图片，Noise2Void只需要单张图片训练，但不能充分利用图片信息，准确率稍低。在冷冻电镜实验中通常无法获得完全理想的干净数据集，但是可以获取Noise2Noise训练所需的数据集，因此我们采取了Noise2Noise训练方案。

在本次计算实验过程中，我们利用前面的理论结构构造图作为参考图片，对它多次施加同种类的随机噪声，由此可以构造出Noise2Noise需要的多对训练集图片。

### 损失函数

损失函数是衡量模型预测结果与真实值之间差距的数值指标。深度学习的训练过程可以看做通过调整网络参数，使得网络的输出预测值与真实值之间的Loss最小。Loss函数的选择直接影响了模型的预测准确率、泛化能力、收敛速度等重要指标。
常用的Loss函数有：

平均绝对误差函数(L1 Loss)
平均绝对误差衡量的是模型预测值与真实值差值的绝对值的均值。计算公式如下：
![](2022-04-08-18-09-55.png)
函数的图像如图：
![](2022-04-08-18-10-33.png)
L1 Loss曲线是连续的，除0以外都可导，且导数为常数。这种特性使得在训练过程中对于各种输入值都能得到稳定的梯度，避免了梯度爆炸的现象，解的鲁棒性较强。但是同时，常数梯度值会导致对于较大的损失值也采用了同样的梯度，影响了训练的收敛速率。

均方误差 （L2 Loss）
均方误差衡量的是模型预测值与真实样本差值平方的均值，其计算公式如下：
![](2022-04-08-18-17-44.png)
其图像如下:
![](2022-04-08-18-18-05.png)
MSE的函数光滑连续且每一处均可导。MSE能较好地反映出误差和梯度的关系：误差较大时，梯度也较大，这提高了训练过程的收敛速度。
除了影响模型的收敛速率外，收敛函数的选择也也会影响到网络模型的效果。
L1 Loss和L2 loss学习到的信息并不完全相同。当Loss函数达到最小是，L1 loss倾向于学习到数据集的中值；而L2 loss倾向于学习到数据集的平均值。Noise2Noise方法的基本假设就是训练集具有0均值的噪声，因此在Noise2Noise的学习任务中采用L2 loss函数更合理。

### 参数优化策略
深度学习的根本任务是将网络参数优化，使得Loss最小。优化算法的选择直接影响了模型运算时间和运算量。

https://sirlis.gitee.io/deep-learning-basic-hp-and-opt/

由于神经网络结构复杂，对应的函数形式无法解析地求出极值，学界普遍采用以梯度下降为基础的数值优化手段。

梯度下降法的数学解释如下：
如果函数F(x)在点a处可微而且有定义，那么函数F(x)在a点沿着梯度相反的方向-delta(F(a))下降得最快。通过迭代过程，最终可以下降到导数为0的点。当此点是全局最小值时，模型参数便收敛到了最优解。
传统的优化器采用的是批量梯度下降策略（BGD），在每一轮训练中，需要对全部样本进行梯度计算，然后取平均值进行权值更新。当数据集增大时，这种策略带来了极大的运算量；同时，朴素的梯度下降有可能收敛到局部最小值（鞍点）。为了提高优化速度，并且避免收敛到局部极值吗，学界发展出了几种不同的优化器。

#### SGD优化器
SGD优化器相比BGD优化器的主要区别在于，SGD并不会在每一轮计算中计算所有样本的梯度，而是在每轮计算中随机抽取其中一部分样本来计算梯度。在计算前期，这种策略可以大大加快计算过程，使得Loss收敛到较低的水平。但是在计算后期，当我们需要Loss收敛到最小值时，由于样本的随机性，Loss可能会在最低值附近发生震荡跳跃，无法有有限迭代次数内收敛。

#### Adam优化器
https://www.cnblogs.com/rezero/p/13290409.html

借鉴物理学中动量的概念，人们发展出了带动量的优化器，通过使用历史梯度的指数均值来调整参数更新方向，从而使得Loss函数发生震荡时，更新步长减小，使得Loss可以在有限步计算中收敛。

目前，Adam优化器是学界应用最广泛的参数优化策略，因此本文也将采取此优化器进行训练。

### 实验部分 

加噪图片：

![image-20220409215203992](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220409215203992.png)

去噪图片

![image-20220409215214951](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20220409215214951.png)

| 指标 | Noised | Denoised |
| ---- | ------ | -------- |
| MSE  | 0.983  | 0.015    |
| PSNR | 0.072  | 17.982   |
| SSIM | 0.006  | 0.5543   |

可以看到Noise2Noise方法相比滤波器方法，在所有降噪指标上都有巨大提升。直观上看，对于高信噪比的图片，Noise2Noise基本保留了实体形状，而且对比度高，并且减弱了原来的噪点。

